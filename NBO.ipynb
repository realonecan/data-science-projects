{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b81b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import required libraries ---\n",
    "\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings, gc\n",
    "warnings.filterwarnings('ignore')\n",
    "gc.collect()\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "# Database and data processing\n",
    "from sqlalchemy import create_engine\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score\n",
    "from category_encoders import TargetEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Models\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model interpretation\n",
    "import shap\n",
    "\n",
    "# --- General settings ---\n",
    "pd.set_option('display.max_columns', 50)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# --- Helper dictionary for datasets ---\n",
    "# This will let us store all tables (old + new) in one place\n",
    "data_dict = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7c6529",
   "metadata": {},
   "source": [
    "##Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afae8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded deposits: 3,377,481 rows, 14 columns\n",
      "Loaded clients: 3,000,000 rows, 8 columns\n",
      "Loaded transactions: 3,000,000 rows, 12 columns\n",
      "Loaded firebase: 3,000,000 rows, 12 columns\n",
      "Data preparation completed.\n"
     ]
    }
   ],
   "source": [
    "# Database connection (replace with your own credentials)\n",
    "# Example:\n",
    "# engine = create_engine(\"postgresql://user:password@host:port/dbname\")\n",
    "# data_dict = {\n",
    "#     'deposits': pd.read_sql(\"SELECT * FROM deposits\", engine),\n",
    "#     ...\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9039900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "--- DEPOSITS ---\n",
      "============================================================\n",
      "Shape: (3340742, 14)\n",
      "\n",
      "Column dtypes:\n",
      "date_rep      datetime64[ns]\n",
      "client_id              int32\n",
      "date_open     datetime64[ns]\n",
      "date_close    datetime64[ns]\n",
      "s_ost_vkl            float64\n",
      "obor_db              float64\n",
      "obor_kr              float64\n",
      "ostatok_op           float64\n",
      "dat_old_pr            object\n",
      "rest_bal             float64\n",
      "proc_all             float64\n",
      "num                    int32\n",
      "state_sd                int8\n",
      "state_name            object\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "    date_rep  client_id  date_open date_close   s_ost_vkl     obor_db  \\\n",
      "0 2025-01-06   60754369 2024-07-31 2025-01-31  90000000.0  90000000.0   \n",
      "1 2025-01-27   60754369 2024-07-31 2025-01-31  90000000.0  90000000.0   \n",
      "2 2025-02-24   60918953 2024-09-10 2026-10-10   2000000.0   2000000.0   \n",
      "\n",
      "   obor_kr  ostatok_op  dat_old_pr    rest_bal     proc_all   num  state_sd  \\\n",
      "0      0.0  90000000.0  2024-07-31   355068.47   9350137.00  8799         1   \n",
      "1      0.0  90000000.0  2024-07-31  1597808.10  10592877.00  8799         1   \n",
      "2      0.0   2000000.0  2024-09-10    20712.30    245589.02  8652         1   \n",
      "\n",
      "  state_name  \n",
      "0   Активный  \n",
      "1   Активный  \n",
      "2   Активный  \n",
      "\n",
      "============================================================\n",
      "--- CLIENTS ---\n",
      "============================================================\n",
      "Shape: (3000000, 8)\n",
      "\n",
      "Column dtypes:\n",
      "Column1                       int32\n",
      "client_id                     int32\n",
      "birthday             datetime64[ns]\n",
      "code_class_credit            object\n",
      "state                       float32\n",
      "code_citizenship            float32\n",
      "birth_place                  object\n",
      "code_gender                    int8\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "   Column1  client_id   birthday code_class_credit  state  code_citizenship  \\\n",
      "0  1400054   61146344 2007-10-28                      2.0             860.0   \n",
      "1  1400055   61146365 1975-05-14                      2.0             860.0   \n",
      "2  1400056   61146424 1994-12-05                      2.0             860.0   \n",
      "\n",
      "        birth_place  code_gender  \n",
      "0  MIRISHKOR TUMANI            1  \n",
      "1   QARAOZEK RAYONI            1  \n",
      "2        TOJIKISTON            2  \n",
      "\n",
      "============================================================\n",
      "--- TRANSACTIONS ---\n",
      "============================================================\n",
      "Shape: (2942216, 12)\n",
      "\n",
      "Column dtypes:\n",
      "Column1               int32\n",
      "id                  float64\n",
      "v_date       datetime64[ns]\n",
      "s_in                float64\n",
      "s_out               float64\n",
      "dt                  float64\n",
      "ct                  float64\n",
      "bs_in               float64\n",
      "bs_out              float64\n",
      "bdt                 float64\n",
      "bct                 float64\n",
      "client_id             int32\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "   Column1            id     v_date        s_in      s_out         dt  \\\n",
      "0  2149520  1.630900e+19 2025-01-06  -3359516.0 -3628277.0   268761.0   \n",
      "1  2149521  1.630900e+19 2025-01-06 -34185728.0        0.0  2357636.0   \n",
      "2  2149522  1.250500e+19 2025-01-06         0.0        0.0  6180726.0   \n",
      "\n",
      "           ct       bs_in     bs_out        bdt         bct  client_id  \n",
      "0         0.0  -3359516.0 -3628277.0   268761.0         0.0   60148440  \n",
      "1  36543360.0 -34185728.0        0.0  2357636.0  36543360.0   90233527  \n",
      "2   6180726.0         0.0        0.0  6180726.0   6180726.0   90508893  \n",
      "\n",
      "============================================================\n",
      "--- FIREBASE ---\n",
      "============================================================\n",
      "Shape: (2968814, 12)\n",
      "\n",
      "Column dtypes:\n",
      "Column1                           int32\n",
      "client_id                         int32\n",
      "platform                         object\n",
      "event_date               datetime64[ns]\n",
      "manual_source                    object\n",
      "operating_system                 object\n",
      "language                         object\n",
      "city                             object\n",
      "current_screen                   object\n",
      "engaged_session_event           float32\n",
      "engagement_time_msec            float32\n",
      "event_name                       object\n",
      "dtype: object\n",
      "\n",
      "First 3 rows:\n",
      "   Column1  client_id platform event_date manual_source operating_system  \\\n",
      "0  1052887   60227577  ANDROID 2025-01-17      firebase          Android   \n",
      "1  1052888   90249491  ANDROID 2025-01-21      firebase          Android   \n",
      "2  1052889   61288870  ANDROID 2025-01-17      firebase          Android   \n",
      "\n",
      "  language        city   current_screen  engaged_session_event  \\\n",
      "0    en-us    Tashkent  Contact fill in                    1.0   \n",
      "1    uz-uz    Tashkent         payments                    1.0   \n",
      "2    uz-uz  Strasbourg       Cards card                    NaN   \n",
      "\n",
      "   engagement_time_msec                           event_name  \n",
      "0                   NaN  contact_fill_in_cardselector_select  \n",
      "1                   NaN                payments_search_input  \n",
      "2                   NaN                      cards_card_view  \n"
     ]
    }
   ],
   "source": [
    "for alias, df in data_dict.items():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'--- {alias.upper()} ---')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(f'Shape: {df.shape}')\n",
    "    print(f'\\nColumn dtypes:')\n",
    "    print(df.dtypes)\n",
    "    print(f'\\nFirst 3 rows:')\n",
    "    print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0818178e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Missing values in DEPOSITS:\n",
      "============================================================\n",
      "          Missing_Count  Missing_Percentage\n",
      "proc_all          27938            0.836281\n",
      "rest_bal          26339            0.788418\n",
      "\n",
      "============================================================\n",
      "Missing values in CLIENTS:\n",
      "============================================================\n",
      "                  Missing_Count  Missing_Percentage\n",
      "state                       134            0.004467\n",
      "code_citizenship            126            0.004200\n",
      "\n",
      "============================================================\n",
      "Missing values in TRANSACTIONS:\n",
      "============================================================\n",
      "Empty DataFrame\n",
      "Columns: [Missing_Count, Missing_Percentage]\n",
      "Index: []\n",
      "\n",
      "============================================================\n",
      "Missing values in FIREBASE:\n",
      "============================================================\n",
      "                       Missing_Count  Missing_Percentage\n",
      "engagement_time_msec         2887775           97.270324\n",
      "engaged_session_event         443910           14.952436\n"
     ]
    }
   ],
   "source": [
    "for alias, df in data_dict.items():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Missing values in {alias.upper()}:')\n",
    "    print(f'{\"=\"*60}')\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing,\n",
    "        'Missing_Percentage': missing_pct\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "596589a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Numeric statistics for DEPOSITS:\n",
      "============================================================\n",
      "                            date_rep     client_id  \\\n",
      "count                        3340742  3.340742e+06   \n",
      "mean   2025-02-13 23:10:51.853988608  7.653260e+07   \n",
      "min              2025-01-03 00:00:00  6.000009e+07   \n",
      "25%              2025-01-23 00:00:00  6.050227e+07   \n",
      "50%              2025-02-13 00:00:00  9.001984e+07   \n",
      "75%              2025-03-06 00:00:00  9.025688e+07   \n",
      "max              2025-03-28 00:00:00  9.097304e+07   \n",
      "std                              NaN  1.485554e+07   \n",
      "\n",
      "                           date_open                     date_close  \\\n",
      "count                        3340742                        3340742   \n",
      "mean   2024-06-22 04:35:03.596866816  2026-04-02 17:06:55.638682368   \n",
      "min              2022-08-27 00:00:00            2025-01-03 00:00:00   \n",
      "25%              2024-02-14 00:00:00            2025-10-19 00:00:00   \n",
      "50%              2024-08-11 00:00:00            2026-04-09 00:00:00   \n",
      "75%              2024-12-03 00:00:00            2026-10-04 00:00:00   \n",
      "max              2025-03-28 00:00:00            2028-03-26 00:00:00   \n",
      "std                              NaN                            NaN   \n",
      "\n",
      "          s_ost_vkl       obor_db       obor_kr    ostatok_op      rest_bal  \\\n",
      "count  3.340742e+06  3.340742e+06  3.340742e+06  3.340742e+06  3.314403e+06   \n",
      "mean   8.478521e+07  2.196999e+08  1.200030e+08  9.971062e+07  9.898262e+05   \n",
      "min    1.000000e+02  1.000000e+02  0.000000e+00  0.000000e+00  0.000000e+00   \n",
      "25%    2.000000e+06  1.000000e+07  0.000000e+00  1.500000e+06  9.825370e+03   \n",
      "50%    1.070000e+07  4.500000e+07  0.000000e+00  1.599045e+07  1.105145e+05   \n",
      "75%    5.642793e+07  1.370089e+08  1.707000e+07  7.337881e+07  5.912441e+05   \n",
      "max    8.250000e+10  5.376396e+11  3.371644e+11  2.729041e+11  5.547945e+09   \n",
      "std    7.213457e+08  3.068293e+09  2.515958e+09  1.367144e+09  1.602744e+07   \n",
      "\n",
      "           proc_all           num      state_sd  \n",
      "count  3.312804e+06  3.340742e+06  3.340742e+06  \n",
      "mean   1.543386e+07  1.793124e+04  1.006530e+00  \n",
      "min    1.000000e-02  1.000000e+00  1.000000e+00  \n",
      "25%    4.194710e+05  3.926000e+03  1.000000e+00  \n",
      "50%    2.237470e+06  1.155200e+04  1.000000e+00  \n",
      "75%    9.457532e+06  3.166900e+04  1.000000e+00  \n",
      "max    4.187403e+10  5.897200e+04  6.000000e+00  \n",
      "std    1.764859e+08  1.720641e+04  1.152707e-01  \n",
      "\n",
      "============================================================\n",
      "Numeric statistics for CLIENTS:\n",
      "============================================================\n",
      "            Column1     client_id                       birthday  \\\n",
      "count  3.000000e+06  3.000000e+06                        3000000   \n",
      "mean   1.500000e+06  7.036655e+07  1991-11-28 06:04:07.824000256   \n",
      "min    0.000000e+00  2.007300e+05            1922-11-01 00:00:00   \n",
      "25%    7.499998e+05  6.074484e+07            1985-05-04 00:00:00   \n",
      "50%    1.500000e+06  6.150169e+07            1994-02-21 00:00:00   \n",
      "75%    2.249999e+06  9.022203e+07            2001-01-07 00:00:00   \n",
      "max    2.999999e+06  9.097310e+07            2020-01-24 00:00:00   \n",
      "std    8.660255e+05  1.432354e+07                            NaN   \n",
      "\n",
      "              state  code_citizenship   code_gender  \n",
      "count  2.999866e+06      2.999874e+06  3.000000e+06  \n",
      "mean   2.000387e+00      8.596694e+02  1.354635e+00  \n",
      "min    1.000000e+00      4.000000e+00  1.000000e+00  \n",
      "25%    2.000000e+00      8.600000e+02  1.000000e+00  \n",
      "50%    2.000000e+00      8.600000e+02  1.000000e+00  \n",
      "75%    2.000000e+00      8.600000e+02  2.000000e+00  \n",
      "max    6.000000e+00      9.990000e+02  2.000000e+00  \n",
      "std    2.129642e-02      1.038513e+01  4.784027e-01  \n",
      "\n",
      "============================================================\n",
      "Numeric statistics for TRANSACTIONS:\n",
      "============================================================\n",
      "            Column1            id                         v_date  \\\n",
      "count  2.942216e+06  2.942216e+06                        2942216   \n",
      "mean   1.499605e+06  2.805775e+19  2025-01-04 22:17:50.367095296   \n",
      "min    0.000000e+00  1.250100e+19            2025-01-03 00:00:00   \n",
      "25%    7.495888e+05  1.630900e+19            2025-01-04 00:00:00   \n",
      "50%    1.499424e+06  1.637700e+19            2025-01-04 00:00:00   \n",
      "75%    2.249431e+06  2.261800e+19            2025-01-06 00:00:00   \n",
      "max    2.999999e+06  9.639700e+19            2025-01-21 00:00:00   \n",
      "std    8.659871e+05  2.540484e+19                            NaN   \n",
      "\n",
      "               s_in         s_out            dt            ct         bs_in  \\\n",
      "count  2.942216e+06  2.942216e+06  2.942216e+06  2.942216e+06  2.942216e+06   \n",
      "mean   7.803999e+06  8.065868e+06  1.104574e+08  1.107192e+08  1.038397e+07   \n",
      "min   -1.546733e+11 -4.900000e+11  0.000000e+00  0.000000e+00 -3.038576e+11   \n",
      "25%   -1.070137e+07 -1.410000e+07  0.000000e+00  0.000000e+00 -1.219707e+07   \n",
      "50%    0.000000e+00 -1.652950e+04  9.402740e+05  0.000000e+00  0.000000e+00   \n",
      "75%    5.000000e+00  1.647250e+03  1.010000e+07  1.301498e+07  1.157000e+03   \n",
      "max    1.041109e+13  1.046386e+13  2.999295e+12  2.000000e+12  1.041109e+13   \n",
      "std    7.851966e+09  7.980028e+09  3.369324e+09  3.248460e+09  1.074363e+10   \n",
      "\n",
      "             bs_out           bdt           bct     client_id  \n",
      "count  2.942216e+06  2.942216e+06  2.942216e+06  2.942216e+06  \n",
      "mean   1.106235e+07  1.142365e+08  1.149148e+08  7.219404e+07  \n",
      "min   -4.900000e+11  0.000000e+00  0.000000e+00  2.026090e+05  \n",
      "25%   -1.583581e+07  0.000000e+00  0.000000e+00  6.040809e+07  \n",
      "50%   -6.866300e+04  9.932860e+05  0.000000e+00  6.094249e+07  \n",
      "75%    8.995225e+04  1.065853e+07  1.388401e+07  9.034823e+07  \n",
      "max    1.046386e+13  2.999295e+12  2.000000e+12  9.097310e+07  \n",
      "std    1.084841e+10  3.476644e+09  3.358587e+09  1.803325e+07  \n",
      "\n",
      "============================================================\n",
      "Numeric statistics for FIREBASE:\n",
      "============================================================\n",
      "            Column1     client_id                     event_date  \\\n",
      "count  2.968814e+06  2.968814e+06                        2968814   \n",
      "mean   1.499505e+06  7.009602e+07  2025-01-15 18:45:18.976669184   \n",
      "min    0.000000e+00  6.000009e+07            2025-01-02 00:00:00   \n",
      "25%    7.495072e+05  6.069117e+07            2025-01-13 00:00:00   \n",
      "50%    1.499236e+06  6.123685e+07            2025-01-16 00:00:00   \n",
      "75%    2.249386e+06  9.016160e+07            2025-01-21 00:00:00   \n",
      "max    2.999999e+06  9.097309e+07            2025-01-31 00:00:00   \n",
      "std    8.659676e+05  1.371348e+07                            NaN   \n",
      "\n",
      "       engaged_session_event  engagement_time_msec  \n",
      "count              2524904.0          8.103900e+04  \n",
      "mean                     1.0          2.633962e+04  \n",
      "min                      1.0          0.000000e+00  \n",
      "25%                      1.0          7.905000e+02  \n",
      "50%                      1.0          8.028000e+03  \n",
      "75%                      1.0          2.698500e+04  \n",
      "max                      1.0          3.600000e+06  \n",
      "std                      0.0          7.858513e+04  \n"
     ]
    }
   ],
   "source": [
    "for alias, df in data_dict.items():\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Numeric statistics for {alias.upper()}:')\n",
    "    print(f'{\"=\"*60}')\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a2a8c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEPOSITS - client_id analysis:\n",
      "============================================================\n",
      "Total rows: 3,340,742\n",
      "Unique client_ids: 34,142\n",
      "Rows per client (avg): 97.85\n",
      "Min client_id: 60000090\n",
      "Max client_id: 90973045\n",
      "\n",
      "============================================================\n",
      "CLIENTS - client_id analysis:\n",
      "============================================================\n",
      "Total rows: 3,000,000\n",
      "Unique client_ids: 3,000,000\n",
      "Rows per client (avg): 1.00\n",
      "Min client_id: 200730\n",
      "Max client_id: 90973102\n",
      "\n",
      "============================================================\n",
      "TRANSACTIONS - client_id analysis:\n",
      "============================================================\n",
      "Total rows: 2,942,216\n",
      "Unique client_ids: 502,349\n",
      "Rows per client (avg): 5.86\n",
      "Min client_id: 202609\n",
      "Max client_id: 90973100\n",
      "\n",
      "============================================================\n",
      "FIREBASE - client_id analysis:\n",
      "============================================================\n",
      "Total rows: 2,968,814\n",
      "Unique client_ids: 110,258\n",
      "Rows per client (avg): 26.93\n",
      "Min client_id: 60000090\n",
      "Max client_id: 90973089\n"
     ]
    }
   ],
   "source": [
    "for alias, df in data_dict.items():\n",
    "    if 'client_id' in df.columns:\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'{alias.upper()} - client_id analysis:')\n",
    "        print(f'{\"=\"*60}')\n",
    "        print(f'Total rows: {len(df):,}')\n",
    "        print(f'Unique client_ids: {df[\"client_id\"].nunique():,}')\n",
    "        print(f'Rows per client (avg): {len(df) / df[\"client_id\"].nunique():.2f}')\n",
    "        print(f'Min client_id: {df[\"client_id\"].min()}')\n",
    "        print(f'Max client_id: {df[\"client_id\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd10639b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique clients in deposits: 34,142\n",
      "Unique clients in clients table: 3,000,000\n",
      "Unique clients in transactions: 502,349\n",
      "Unique clients in firebase: 110,258\n",
      "\n",
      "--- Overlap Analysis ---\n",
      "Deposits clients NOT in clients table: 0\n",
      "Clients table NOT in deposits: 2,965,858\n",
      "Transactions clients NOT in clients table: 0\n",
      "Firebase clients NOT in clients table: 0\n"
     ]
    }
   ],
   "source": [
    "# Get unique client sets\n",
    "deposits_clients = set(data_dict['deposits']['client_id'].unique())\n",
    "clients_clients = set(data_dict['clients']['client_id'].unique())\n",
    "trans_clients = set(data_dict['transactions']['client_id'].unique())\n",
    "firebase_clients = set(data_dict['firebase']['client_id'].unique())\n",
    "\n",
    "print(f'Unique clients in deposits: {len(deposits_clients):,}')\n",
    "print(f'Unique clients in clients table: {len(clients_clients):,}')\n",
    "print(f'Unique clients in transactions: {len(trans_clients):,}')\n",
    "print(f'Unique clients in firebase: {len(firebase_clients):,}')\n",
    "\n",
    "print(f'\\n--- Overlap Analysis ---')\n",
    "print(f'Deposits clients NOT in clients table: {len(deposits_clients - clients_clients):,}')\n",
    "print(f'Clients table NOT in deposits: {len(clients_clients - deposits_clients):,}')\n",
    "print(f'Transactions clients NOT in clients table: {len(trans_clients - clients_clients):,}')\n",
    "print(f'Firebase clients NOT in clients table: {len(firebase_clients - clients_clients):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9295c76b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal validation in DEPOSITS:\n",
      "Total rows: 3,340,742\n",
      "Rows where date_rep < date_open: 0\n",
      "Rows where date_rep > date_close: 0\n",
      "Rows where date_open >= date_close: 0\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "\n",
    "# Check if date_rep is within date_open and date_close\n",
    "print('Temporal validation in DEPOSITS:')\n",
    "print(f'Total rows: {len(deposits):,}')\n",
    "\n",
    "# date_rep should be >= date_open\n",
    "invalid_open = (deposits['date_rep'] < deposits['date_open']).sum()\n",
    "print(f'Rows where date_rep < date_open: {invalid_open:,}')\n",
    "\n",
    "# date_rep should be <= date_close\n",
    "invalid_close = (deposits['date_rep'] > deposits['date_close']).sum()\n",
    "print(f'Rows where date_rep > date_close: {invalid_close:,}')\n",
    "\n",
    "# date_open should be < date_close\n",
    "invalid_range = (deposits['date_open'] >= deposits['date_close']).sum()\n",
    "print(f'Rows where date_open >= date_close: {invalid_range:,}')\n",
    "\n",
    "# Show examples if any violations exist\n",
    "if invalid_open > 0:\n",
    "    print('\\nSample rows where date_rep < date_open:')\n",
    "    print(deposits[deposits['date_rep'] < deposits['date_open']][['client_id', 'date_rep', 'date_open', 'date_close']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e09faaa",
   "metadata": {},
   "source": [
    "CRITICAL: Client_ID Mismatches\n",
    "\n",
    "402 clients in deposits NOT in clients table (1.16% of deposit clients)\n",
    "8,026 clients in transactions NOT in clients table (1.57% of transaction clients)\n",
    "1,279 clients in firebase NOT in clients table (1.15% of firebase clients)\n",
    "\n",
    "Temporal Anomaly: 840 deposits where date_rep > date_close (reporting date after account closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "860b256b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of 0 deposit client_ids NOT in clients table:\n",
      "[]\n",
      "\n",
      "Deposit rows affected: 0 (0.00%)\n",
      "\n",
      "Sample of affected deposit records:\n",
      "Empty DataFrame\n",
      "Columns: [client_id, date_rep, date_open, date_close, ostatok_op]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Find deposits clients not in clients table\n",
    "deposits_not_in_clients = deposits_clients - clients_clients\n",
    "print(f'Sample of {len(deposits_not_in_clients)} deposit client_ids NOT in clients table:')\n",
    "print(sorted(list(deposits_not_in_clients))[:20])\n",
    "\n",
    "# Check how many deposit rows this affects\n",
    "deposits = data_dict['deposits']\n",
    "affected_deposits = deposits[deposits['client_id'].isin(deposits_not_in_clients)]\n",
    "print(f'\\nDeposit rows affected: {len(affected_deposits):,} ({len(affected_deposits)/len(deposits)*100:.2f}%)')\n",
    "print(f'\\nSample of affected deposit records:')\n",
    "print(affected_deposits[['client_id', 'date_rep', 'date_open', 'date_close', 'ostatok_op']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42cd6d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deposits where date_rep > date_close: 0\n",
      "Unique clients affected: 0\n",
      "\n",
      "Days beyond closure (statistics):\n",
      "count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "dtype: float64\n",
      "\n",
      "Sample records:\n",
      "Empty DataFrame\n",
      "Columns: [client_id, date_rep, date_open, date_close, ostatok_op]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "invalid_date_deposits = deposits[deposits['date_rep'] > deposits['date_close']]\n",
    "\n",
    "print(f'Deposits where date_rep > date_close: {len(invalid_date_deposits):,}')\n",
    "print(f'Unique clients affected: {invalid_date_deposits[\"client_id\"].nunique():,}')\n",
    "print(f'\\nDays beyond closure (statistics):')\n",
    "days_beyond = (invalid_date_deposits['date_rep'] - invalid_date_deposits['date_close']).dt.days\n",
    "print(days_beyond.describe())\n",
    "\n",
    "print(f'\\nSample records:')\n",
    "print(invalid_date_deposits[['client_id', 'date_rep', 'date_open', 'date_close', 'ostatok_op']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35d50350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_sd distribution:\n",
      "state_sd\n",
      "1    3329924\n",
      "3      10758\n",
      "6         60\n",
      "Name: count, dtype: int64\n",
      "\n",
      "state_name distribution:\n",
      "state_name\n",
      "Активный          3329924\n",
      "Договор закрыт      10758\n",
      "Залоговый              60\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Analyzing potential target variable ---\n",
      "Most recent date_rep: 2025-03-28 00:00:00\n",
      "Most recent date_close: 2028-03-26 00:00:00\n",
      "\n",
      "Deposits opened in 2025: 482,088\n",
      "Unique clients with 2025 deposits: 12,258\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "\n",
    "# Check state_sd distribution (likely deposit state)\n",
    "print('state_sd distribution:')\n",
    "print(deposits['state_sd'].value_counts().sort_index())\n",
    "\n",
    "print('\\nstate_name distribution:')\n",
    "print(deposits['state_name'].value_counts())\n",
    "\n",
    "# Check if state_sd could be our target (active/inactive deposit)\n",
    "print('\\n--- Analyzing potential target variable ---')\n",
    "print(f'Most recent date_rep: {deposits[\"date_rep\"].max()}')\n",
    "print(f'Most recent date_close: {deposits[\"date_close\"].max()}')\n",
    "\n",
    "# Are there deposits that opened recently (potential new deposits)?\n",
    "recent_opens = deposits[deposits['date_open'] >= '2025-01-01']\n",
    "print(f'\\nDeposits opened in 2025: {len(recent_opens):,}')\n",
    "print(f'Unique clients with 2025 deposits: {recent_opens[\"client_id\"].nunique():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f015eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deposits per client distribution:\n",
      "count    34142.000000\n",
      "mean        97.848456\n",
      "std        185.800848\n",
      "min          1.000000\n",
      "25%         60.000000\n",
      "50%         60.000000\n",
      "75%        120.000000\n",
      "max      17428.000000\n",
      "Name: num_deposits, dtype: float64\n",
      "\n",
      "Clients by number of deposits:\n",
      "num_deposits\n",
      "1     361\n",
      "2     264\n",
      "3     167\n",
      "4     170\n",
      "5     189\n",
      "6     135\n",
      "7     161\n",
      "8     119\n",
      "9     173\n",
      "10    139\n",
      "11    125\n",
      "12    152\n",
      "13    200\n",
      "14    106\n",
      "15    116\n",
      "16    111\n",
      "17    138\n",
      "18    128\n",
      "19     93\n",
      "20     86\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Clients with 2+ deposits: 33,781\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "\n",
    "# Group by client to understand deposit patterns\n",
    "deposits_per_client = deposits.groupby('client_id').agg({\n",
    "    'num': 'count',  # number of deposit records\n",
    "    'date_open': ['min', 'max'],  # first and last deposit opened\n",
    "    'date_rep': 'max'  # latest reporting date\n",
    "}).reset_index()\n",
    "\n",
    "deposits_per_client.columns = ['client_id', 'num_deposits', 'first_deposit', 'last_deposit', 'last_report_date']\n",
    "\n",
    "print('Deposits per client distribution:')\n",
    "print(deposits_per_client['num_deposits'].describe())\n",
    "\n",
    "print('\\nClients by number of deposits:')\n",
    "print(deposits_per_client['num_deposits'].value_counts().sort_index().head(20))\n",
    "\n",
    "# Clients who opened multiple deposits (potential repeat behavior)\n",
    "print(f'\\nClients with 2+ deposits: {(deposits_per_client[\"num_deposits\"] >= 2).sum():,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dd8554",
   "metadata": {},
   "source": [
    "Critical Findings Analysis:\n",
    "Data Quality Issues:\n",
    "\n",
    "35,899 deposit rows (1.06%) lack client demographic data - these 402 clients are missing from clients table\n",
    "840 rows reporting AFTER closure - accounts closed in 2023 still being reported in 2025 (600+ days beyond closure)\n",
    "These appear to be \"Арестован\" (Arrested/Frozen) accounts (state_sd=2) - likely valid edge case\n",
    "\n",
    "Target Understanding:\n",
    "\n",
    "state_name breakdown: Активный (Active: 99.6%), Договор закрыт (Closed: 0.3%), Арестован (Arrested: 0.02%), Залоговый (Collateral: 0.002%)\n",
    "12,384 clients opened NEW deposits in 2025 - this is our positive class for NBO!\n",
    "34,186 clients (99%) have 2+ deposits - strong repeat behavior pattern\n",
    "\n",
    "Key Insight: Your target should be \"Will client open a NEW deposit in the prediction window?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2f41602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique deposit accounts per client:\n",
      "num_unique_deposits\n",
      "1     20581\n",
      "2      7670\n",
      "3      2898\n",
      "4      1217\n",
      "5       622\n",
      "6       329\n",
      "7       202\n",
      "8       132\n",
      "9        96\n",
      "10       76\n",
      "11       61\n",
      "12       47\n",
      "13       32\n",
      "14       23\n",
      "15       13\n",
      "16       21\n",
      "17       14\n",
      "18        6\n",
      "19        8\n",
      "20       13\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Max deposits opened on same day by a client:\n",
      "max_same_day\n",
      "1        380\n",
      "2        283\n",
      "3        172\n",
      "4        179\n",
      "5        200\n",
      "        ... \n",
      "1260       1\n",
      "2100       1\n",
      "2340       1\n",
      "2820       1\n",
      "11880      1\n",
      "Name: count, Length: 192, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "\n",
    "# For each client, find their deposit opening events over time\n",
    "client_deposit_timeline = deposits.groupby('client_id')['date_open'].apply(list).reset_index()\n",
    "client_deposit_timeline['num_unique_deposits'] = client_deposit_timeline['date_open'].apply(lambda x: len(set(x)))\n",
    "\n",
    "print('Unique deposit accounts per client:')\n",
    "print(client_deposit_timeline['num_unique_deposits'].value_counts().sort_index().head(20))\n",
    "\n",
    "# Check if clients open multiple deposits on same day\n",
    "client_deposit_timeline['max_same_day'] = client_deposit_timeline['date_open'].apply(\n",
    "    lambda x: max([x.count(date) for date in set(x)])\n",
    ")\n",
    "print('\\nMax deposits opened on same day by a client:')\n",
    "print(client_deposit_timeline['max_same_day'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3aa49b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date ranges in dataset:\n",
      "date_rep (reporting/snapshot): 2025-01-03 00:00:00 to 2025-03-28 00:00:00\n",
      "date_open (account opening): 2022-08-27 00:00:00 to 2025-03-28 00:00:00\n",
      "date_close (maturity): 2025-01-03 00:00:00 to 2028-03-26 00:00:00\n",
      "\n",
      "Unique date_rep values: 60\n",
      "\n",
      "date_rep value counts (first 20):\n",
      "date_rep\n",
      "2025-01-03    52108\n",
      "2025-01-04    52274\n",
      "2025-01-06    52483\n",
      "2025-01-07    52614\n",
      "2025-01-08    52730\n",
      "2025-01-09    52881\n",
      "2025-01-10    53014\n",
      "2025-01-13    53166\n",
      "2025-01-14    53273\n",
      "2025-01-15    53428\n",
      "2025-01-16    53587\n",
      "2025-01-17    53666\n",
      "2025-01-20    53864\n",
      "2025-01-21    53920\n",
      "2025-01-22    54037\n",
      "2025-01-23    54159\n",
      "2025-01-24    54233\n",
      "2025-01-27    54349\n",
      "2025-01-28    54405\n",
      "2025-01-29    54508\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total days in date_rep range: 85\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "\n",
    "print('Date ranges in dataset:')\n",
    "print(f'date_rep (reporting/snapshot): {deposits[\"date_rep\"].min()} to {deposits[\"date_rep\"].max()}')\n",
    "print(f'date_open (account opening): {deposits[\"date_open\"].min()} to {deposits[\"date_open\"].max()}')\n",
    "print(f'date_close (maturity): {deposits[\"date_close\"].min()} to {deposits[\"date_close\"].max()}')\n",
    "\n",
    "# Check date_rep distribution - is this daily snapshots?\n",
    "print(f'\\nUnique date_rep values: {deposits[\"date_rep\"].nunique()}')\n",
    "print('\\ndate_rep value counts (first 20):')\n",
    "print(deposits['date_rep'].value_counts().sort_index().head(20))\n",
    "\n",
    "# Are we looking at daily account snapshots?\n",
    "print(f'\\nTotal days in date_rep range: {(deposits[\"date_rep\"].max() - deposits[\"date_rep\"].min()).days + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e138be37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LEAKAGE RISK ASSESSMENT ===\n",
      "\n",
      "1. Checking if date_open (target event) occurs AFTER date_rep (observation time):\n",
      "   Rows with date_open > date_rep: 0\n",
      "   ✓ No future date_open leakage detected\n",
      "\n",
      "2. Checking ostatok_op (balance) for newly opened accounts:\n",
      "   Deposits opened on their date_rep in 2025: 15,412\n",
      "   Non-zero balances on opening day: 14,137\n",
      "   ⚠️  Using balance features from date_rep where date_open=date_rep may leak target!\n",
      "\n",
      "3. Checking deposit-level grain:\n",
      "   Unique deposits (client + date_open): 64,750\n",
      "   Total rows: 3,340,742\n",
      "   Avg snapshots per deposit: 51.59\n",
      "   Max snapshots per deposit: 11880\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "\n",
    "print('=== LEAKAGE RISK ASSESSMENT ===\\n')\n",
    "\n",
    "# Risk 1: Future information in current snapshot\n",
    "print('1. Checking if date_open (target event) occurs AFTER date_rep (observation time):')\n",
    "future_opens = deposits[deposits['date_open'] > deposits['date_rep']]\n",
    "print(f'   Rows with date_open > date_rep: {len(future_opens):,}')\n",
    "if len(future_opens) > 0:\n",
    "    print('   ❌ LEAKAGE RISK: Future deposit openings visible in historical snapshots!')\n",
    "    print(f'   Sample:')\n",
    "    print(future_opens[['client_id', 'date_rep', 'date_open', 'date_close']].head())\n",
    "else:\n",
    "    print('   ✓ No future date_open leakage detected')\n",
    "\n",
    "# Risk 2: Using balance/features from AFTER account opening\n",
    "print('\\n2. Checking ostatok_op (balance) for newly opened accounts:')\n",
    "new_opens_in_window = deposits[\n",
    "    (deposits['date_open'] >= '2025-01-01') & \n",
    "    (deposits['date_open'] == deposits['date_rep'])\n",
    "]\n",
    "print(f'   Deposits opened on their date_rep in 2025: {len(new_opens_in_window):,}')\n",
    "print(f'   Non-zero balances on opening day: {(new_opens_in_window[\"ostatok_op\"] > 0).sum():,}')\n",
    "print('   ⚠️  Using balance features from date_rep where date_open=date_rep may leak target!')\n",
    "\n",
    "# Risk 3: Multiple snapshots per deposit\n",
    "print('\\n3. Checking deposit-level grain:')\n",
    "deposit_id_counts = deposits.groupby(['client_id', 'date_open']).size().reset_index(name='snapshot_count')\n",
    "print(f'   Unique deposits (client + date_open): {len(deposit_id_counts):,}')\n",
    "print(f'   Total rows: {len(deposits):,}')\n",
    "print(f'   Avg snapshots per deposit: {deposit_id_counts[\"snapshot_count\"].mean():.2f}')\n",
    "print(f'   Max snapshots per deposit: {deposit_id_counts[\"snapshot_count\"].max()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e55074c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction dates:\n",
      "\n",
      "Firebase event dates:\n",
      "event_date range: 2025-01-02 00:00:00 to 2025-01-31 00:00:00\n",
      "Unique dates: 28\n",
      "\n",
      "Deposits data ends: 2025-03-28 00:00:00\n",
      "Firebase events after deposit data ends: 0\n"
     ]
    }
   ],
   "source": [
    "transactions = data_dict['transactions']\n",
    "firebase = data_dict['firebase']\n",
    "\n",
    "print('Transaction dates:')\n",
    "if 'date_trn' in transactions.columns:\n",
    "    print(f'date_trn range: {transactions[\"date_trn\"].min()} to {transactions[\"date_trn\"].max()}')\n",
    "    print(f'Unique dates: {transactions[\"date_trn\"].nunique()}')\n",
    "\n",
    "print('\\nFirebase event dates:')\n",
    "if 'event_date' in firebase.columns:\n",
    "    print(f'event_date range: {firebase[\"event_date\"].min()} to {firebase[\"event_date\"].max()}')\n",
    "    print(f'Unique dates: {firebase[\"event_date\"].nunique()}')\n",
    "\n",
    "# Check if any transactions/events occur AFTER deposit data collection ends\n",
    "deposits_end = data_dict['deposits']['date_rep'].max()\n",
    "print(f'\\nDeposits data ends: {deposits_end}')\n",
    "if 'date_trn' in transactions.columns:\n",
    "    future_trans = (transactions['date_trn'] > deposits_end).sum()\n",
    "    print(f'Transactions after deposit data ends: {future_trans:,}')\n",
    "if 'event_date' in firebase.columns:\n",
    "    future_events = (firebase['event_date'] > deposits_end).sum()\n",
    "    print(f'Firebase events after deposit data ends: {future_events:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71f37d",
   "metadata": {},
   "source": [
    " MAJOR LEAKAGE RISKS IDENTIFIED:\n",
    "\n",
    "Daily Account Snapshots Structure:\n",
    "\n",
    "65,516 unique deposits → 3.4M rows = ~51 daily snapshots per deposit\n",
    "This means features like ostatok_op, obor_db, obor_kr on date_rep where date_open = date_rep WILL LEAK THE TARGET\n",
    "\n",
    "\n",
    "Same-Day Opening Balance Leakage:\n",
    "\n",
    "15,550 deposits opened on their date_rep in 2025\n",
    "14,265 (91.7%) have non-zero balance on opening day\n",
    "Using these balance features = directly using the target event outcome as a feature!\n",
    "\n",
    "\n",
    "Firebase Data Limited: Only Jan 2-31, 2025 (vs deposits through Mar 28)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0e7ed8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions columns:\n",
      "['Column1', 'id', 'v_date', 's_in', 's_out', 'dt', 'ct', 'bs_in', 'bs_out', 'bdt', 'bct', 'client_id']\n",
      "\n",
      "Transactions shape: (2942216, 12)\n",
      "\n",
      "First few rows:\n",
      "   Column1            id     v_date        s_in        s_out         dt  \\\n",
      "0  2149520  1.630900e+19 2025-01-06  -3359516.0   -3628277.0   268761.0   \n",
      "1  2149521  1.630900e+19 2025-01-06 -34185728.0          0.0  2357636.0   \n",
      "2  2149522  1.250500e+19 2025-01-06         0.0          0.0  6180726.0   \n",
      "3  2149523  9.638100e+19 2025-01-06         0.0  416000000.0        0.0   \n",
      "4  2149524  2.261800e+19 2025-01-06   1450000.0          0.0  1450000.0   \n",
      "\n",
      "            ct       bs_in       bs_out        bdt          bct  client_id  \n",
      "0          0.0  -3359516.0   -3628277.0   268761.0          0.0   60148440  \n",
      "1   36543360.0 -34185728.0          0.0  2357636.0   36543360.0   90233527  \n",
      "2    6180726.0         0.0          0.0  6180726.0    6180726.0   90508893  \n",
      "3  416000000.0         0.0  416000000.0        0.0  416000000.0   61113426  \n",
      "4          0.0   1450000.0          0.0  1450000.0          0.0   60037697  \n"
     ]
    }
   ],
   "source": [
    "transactions = data_dict['transactions']\n",
    "print('Transactions columns:')\n",
    "print(transactions.columns.tolist())\n",
    "print(f'\\nTransactions shape: {transactions.shape}')\n",
    "print('\\nFirst few rows:')\n",
    "print(transactions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f922f082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client activity in deposits by month:\n",
      "2025-01: 30,630 unique clients\n",
      "2025-02: 31,260 unique clients\n",
      "2025-03: 32,144 unique clients\n",
      "\n",
      "Firebase data coverage (Jan 2025 only):\n",
      "Clients with firebase events: 110,258\n",
      "Clients with deposits: 34,142\n",
      "Overlap: 7,351\n",
      "\n",
      "Clients with transactions: 502,349\n",
      "Overlap with deposits: 29,972\n"
     ]
    }
   ],
   "source": [
    "# Check data availability by time period\n",
    "deposits = data_dict['deposits']\n",
    "transactions = data_dict['transactions']\n",
    "firebase = data_dict['firebase']\n",
    "\n",
    "# For deposits: which clients were active in different periods?\n",
    "print('Client activity in deposits by month:')\n",
    "for month in ['2025-01', '2025-02', '2025-03']:\n",
    "    month_data = deposits[deposits['date_rep'].dt.strftime('%Y-%m') == month]\n",
    "    print(f'{month}: {month_data[\"client_id\"].nunique():,} unique clients')\n",
    "\n",
    "# Firebase coverage\n",
    "print('\\nFirebase data coverage (Jan 2025 only):')\n",
    "print(f'Clients with firebase events: {firebase[\"client_id\"].nunique():,}')\n",
    "print(f'Clients with deposits: {deposits[\"client_id\"].nunique():,}')\n",
    "print(f'Overlap: {len(set(firebase[\"client_id\"]) & set(deposits[\"client_id\"])):,}')\n",
    "\n",
    "# Transaction coverage  \n",
    "print(f'\\nClients with transactions: {transactions[\"client_id\"].nunique():,}')\n",
    "print(f'Overlap with deposits: {len(set(transactions[\"client_id\"]) & set(deposits[\"client_id\"])):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b98c422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROPOSED TEMPORAL SPLIT STRATEGY ===\n",
      "\n",
      "New deposits opened by month in 2025:\n",
      "month\n",
      "2025-01    6442\n",
      "2025-02    5158\n",
      "2025-03    5277\n",
      "Freq: M, Name: count, dtype: int64\n",
      "\n",
      "--- Suggested Split ---\n",
      "Training period: Use data up to 2025-01-31 to predict Feb opens\n",
      "Validation period: Use data up to 2025-02-28 to predict Mar opens\n",
      "Test period: Use data up to 2025-03-15 to predict opens after 3/15\n",
      "\n",
      "Target events (new deposits):\n",
      "Feb 2025 (train target): 5,158 deposits from 4,405 clients\n",
      "Mar 2025 (val target): 5,277 deposits from 4,500 clients\n"
     ]
    }
   ],
   "source": [
    "deposits = data_dict['deposits']\n",
    "\n",
    "# Proposal: Use historical behavior to predict future deposit openings\n",
    "# Example: Use data up to Date X to predict who opens deposit in next 30 days\n",
    "\n",
    "print('=== PROPOSED TEMPORAL SPLIT STRATEGY ===\\n')\n",
    "\n",
    "# Check deposit opening distribution over time\n",
    "deposits_opened = deposits.drop_duplicates(subset=['client_id', 'date_open'])\n",
    "deposits_opened_2025 = deposits_opened[deposits_opened['date_open'] >= '2025-01-01']\n",
    "\n",
    "print('New deposits opened by month in 2025:')\n",
    "deposits_opened_2025['month'] = deposits_opened_2025['date_open'].dt.to_period('M')\n",
    "print(deposits_opened_2025['month'].value_counts().sort_index())\n",
    "\n",
    "print('\\n--- Suggested Split ---')\n",
    "print('Training period: Use data up to 2025-01-31 to predict Feb opens')\n",
    "print('Validation period: Use data up to 2025-02-28 to predict Mar opens')\n",
    "print('Test period: Use data up to 2025-03-15 to predict opens after 3/15')\n",
    "\n",
    "# Check volume for each period\n",
    "train_opens = deposits_opened[(deposits_opened['date_open'] >= '2025-02-01') & \n",
    "                               (deposits_opened['date_open'] <= '2025-02-28')]\n",
    "val_opens = deposits_opened[(deposits_opened['date_open'] >= '2025-03-01') & \n",
    "                             (deposits_opened['date_open'] <= '2025-03-28')]\n",
    "\n",
    "print(f'\\nTarget events (new deposits):')\n",
    "print(f'Feb 2025 (train target): {len(train_opens):,} deposits from {train_opens[\"client_id\"].nunique():,} clients')\n",
    "print(f'Mar 2025 (val target): {len(val_opens):,} deposits from {val_opens[\"client_id\"].nunique():,} clients')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a796b",
   "metadata": {},
   "source": [
    "Beggining\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b677c6e9",
   "metadata": {},
   "source": [
    "Data Cleaning and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63f90181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: DATA CLEANING\n",
      "============================================================\n",
      "\n",
      "1.1 Removing clients without demographic data...\n",
      "Valid clients with demographics: 3,000,000\n",
      "  deposits: removed 0 rows (0.00%)\n",
      "  transactions: removed 0 rows (0.00%)\n",
      "  firebase: removed 0 rows (0.00%)\n",
      "\n",
      "1.2 Removing anomalous deposits (date_rep > date_close)...\n",
      "  Removed 0 anomalous deposit records\n",
      "\n",
      "Cleaned data summary:\n",
      "  Deposits: 3,340,742 rows, 34,142 clients\n",
      "  Clients: 3,000,000 rows\n",
      "  Transactions: 2,942,216 rows, 502,349 clients\n",
      "  Firebase: 2,968,814 rows, 110,258 clients\n",
      "\n",
      "✓ Data cleaning completed!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1: DATA CLEANING AND PREPARATION\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: DATA CLEANING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1.1 Remove clients without demographic data\n",
    "print(\"\\n1.1 Removing clients without demographic data...\")\n",
    "valid_client_ids = set(data_dict['clients']['client_id'].unique())\n",
    "print(f\"Valid clients with demographics: {len(valid_client_ids):,}\")\n",
    "\n",
    "# Filter all tables to only include valid clients\n",
    "for table_name in ['deposits', 'transactions', 'firebase']:\n",
    "    original_len = len(data_dict[table_name])\n",
    "    data_dict[table_name] = data_dict[table_name][\n",
    "        data_dict[table_name]['client_id'].isin(valid_client_ids)\n",
    "    ].copy()\n",
    "    removed = original_len - len(data_dict[table_name])\n",
    "    print(f\"  {table_name}: removed {removed:,} rows ({removed/original_len*100:.2f}%)\")\n",
    "\n",
    "# 1.2 Remove the 840 deposits with date_rep > date_close (data quality issue)\n",
    "print(\"\\n1.2 Removing anomalous deposits (date_rep > date_close)...\")\n",
    "deposits_original = len(data_dict['deposits'])\n",
    "data_dict['deposits'] = data_dict['deposits'][\n",
    "    data_dict['deposits']['date_rep'] <= data_dict['deposits']['date_close']\n",
    "].copy()\n",
    "removed_deposits = deposits_original - len(data_dict['deposits'])\n",
    "print(f\"  Removed {removed_deposits:,} anomalous deposit records\")\n",
    "\n",
    "# 1.3 Create clean references\n",
    "deposits = data_dict['deposits'].copy()\n",
    "clients = data_dict['clients'].copy()\n",
    "transactions = data_dict['transactions'].copy()\n",
    "firebase = data_dict['firebase'].copy()\n",
    "\n",
    "print(f\"\\nCleaned data summary:\")\n",
    "print(f\"  Deposits: {len(deposits):,} rows, {deposits['client_id'].nunique():,} clients\")\n",
    "print(f\"  Clients: {len(clients):,} rows\")\n",
    "print(f\"  Transactions: {len(transactions):,} rows, {transactions['client_id'].nunique():,} clients\")\n",
    "print(f\"  Firebase: {len(firebase):,} rows, {firebase['client_id'].nunique():,} clients\")\n",
    "\n",
    "print(\"\\n✓ Data cleaning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e79d81",
   "metadata": {},
   "source": [
    "Temporal Splits and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e72c4933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 2: TEMPORAL SPLITS & TARGET DEFINITION\n",
      "============================================================\n",
      "\n",
      "2.1 Defining temporal boundaries...\n",
      "\n",
      "TRAIN:\n",
      "  Observation window: 2025-01-01 to 2025-01-31\n",
      "  Prediction window:  2025-02-01 to 2025-02-28\n",
      "\n",
      "VALIDATION:\n",
      "  Observation window: 2025-02-01 to 2025-02-28\n",
      "  Prediction window:  2025-03-01 to 2025-03-28\n",
      "\n",
      "2.2 Identifying target events (new deposit openings)...\n",
      "\n",
      "Positive cases (clients who opened new deposits):\n",
      "  Train (Feb 2025): 4,405 clients\n",
      "  Val (Mar 2025):   4,500 clients\n",
      "\n",
      "2.3 Defining eligible client population...\n",
      "\n",
      "Active clients (eligible for prediction):\n",
      "  Train: 30,630 clients\n",
      "  Val:   31,260 clients\n",
      "\n",
      "2.4 Creating target labels...\n",
      "\n",
      "Train dataset:\n",
      "  Total clients: 30,630\n",
      "  Positive (opened deposit): 2,646 (8.64%)\n",
      "  Negative (did not open): 27,984 (91.36%)\n",
      "\n",
      "Validation dataset:\n",
      "  Total clients: 31,260\n",
      "  Positive (opened deposit): 2,679 (8.57%)\n",
      "  Negative (did not open): 28,581 (91.43%)\n",
      "\n",
      "2.5 Checking for temporal overlap (leakage detection)...\n",
      "  Clients who opened deposits in BOTH Feb and Mar: 754\n",
      "  This is OK - these are repeat customers (not leakage)\n",
      "\n",
      "  Latest date_rep we'll use for train features: 2025-01-31 00:00:00\n",
      "  Latest date_rep we'll use for val features: 2025-02-28 00:00:00\n",
      "  Train prediction starts: 2025-02-01\n",
      "  Val prediction starts: 2025-03-01\n",
      "  ✓ No temporal leakage in train split\n",
      "  ✓ No temporal leakage in val split\n",
      "\n",
      "✓ Temporal splits and targets defined!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2: TEMPORAL SPLIT & TARGET DEFINITION\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: TEMPORAL SPLITS & TARGET DEFINITION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 2.1 Define temporal boundaries\n",
    "print(\"\\n2.1 Defining temporal boundaries...\")\n",
    "\n",
    "# TRAINING SET\n",
    "train_observation_start = '2025-01-01'\n",
    "train_observation_end = '2025-01-31'\n",
    "train_prediction_start = '2025-02-01'\n",
    "train_prediction_end = '2025-02-28'\n",
    "\n",
    "# VALIDATION SET\n",
    "val_observation_start = '2025-02-01'\n",
    "val_observation_end = '2025-02-28'\n",
    "val_prediction_start = '2025-03-01'\n",
    "val_prediction_end = '2025-03-28'\n",
    "\n",
    "print(f\"\\nTRAIN:\")\n",
    "print(f\"  Observation window: {train_observation_start} to {train_observation_end}\")\n",
    "print(f\"  Prediction window:  {train_prediction_start} to {train_prediction_end}\")\n",
    "\n",
    "print(f\"\\nVALIDATION:\")\n",
    "print(f\"  Observation window: {val_observation_start} to {val_observation_end}\")\n",
    "print(f\"  Prediction window:  {val_prediction_start} to {val_prediction_end}\")\n",
    "\n",
    "# 2.2 Identify clients who opened NEW deposits in prediction windows\n",
    "print(\"\\n2.2 Identifying target events (new deposit openings)...\")\n",
    "\n",
    "# Get unique deposit openings (client_id + date_open)\n",
    "deposit_openings = deposits[['client_id', 'date_open']].drop_duplicates()\n",
    "\n",
    "# TRAIN: Clients who opened deposits in Feb 2025\n",
    "train_positive_clients = deposit_openings[\n",
    "    (deposit_openings['date_open'] >= train_prediction_start) &\n",
    "    (deposit_openings['date_open'] <= train_prediction_end)\n",
    "]['client_id'].unique()\n",
    "\n",
    "# VAL: Clients who opened deposits in Mar 2025\n",
    "val_positive_clients = deposit_openings[\n",
    "    (deposit_openings['date_open'] >= val_prediction_start) &\n",
    "    (deposit_openings['date_open'] <= val_prediction_end)\n",
    "]['client_id'].unique()\n",
    "\n",
    "print(f\"\\nPositive cases (clients who opened new deposits):\")\n",
    "print(f\"  Train (Feb 2025): {len(train_positive_clients):,} clients\")\n",
    "print(f\"  Val (Mar 2025):   {len(val_positive_clients):,} clients\")\n",
    "\n",
    "# 2.3 Define eligible client population for modeling\n",
    "print(\"\\n2.3 Defining eligible client population...\")\n",
    "\n",
    "# TRAIN: Clients who were \"active\" during train observation period\n",
    "# Active = had at least one deposit snapshot during observation period\n",
    "train_active_clients = deposits[\n",
    "    (deposits['date_rep'] >= train_observation_start) &\n",
    "    (deposits['date_rep'] <= train_observation_end)\n",
    "]['client_id'].unique()\n",
    "\n",
    "# VAL: Clients who were \"active\" during val observation period\n",
    "val_active_clients = deposits[\n",
    "    (deposits['date_rep'] >= val_observation_start) &\n",
    "    (deposits['date_rep'] <= val_observation_end)\n",
    "]['client_id'].unique()\n",
    "\n",
    "print(f\"\\nActive clients (eligible for prediction):\")\n",
    "print(f\"  Train: {len(train_active_clients):,} clients\")\n",
    "print(f\"  Val:   {len(val_active_clients):,} clients\")\n",
    "\n",
    "# 2.4 Create target labels\n",
    "print(\"\\n2.4 Creating target labels...\")\n",
    "\n",
    "# TRAIN dataset\n",
    "train_targets = pd.DataFrame({\n",
    "    'client_id': train_active_clients,\n",
    "    'target': 0  # Default: did not open deposit\n",
    "})\n",
    "train_targets.loc[train_targets['client_id'].isin(train_positive_clients), 'target'] = 1\n",
    "\n",
    "# VAL dataset\n",
    "val_targets = pd.DataFrame({\n",
    "    'client_id': val_active_clients,\n",
    "    'target': 0\n",
    "})\n",
    "val_targets.loc[val_targets['client_id'].isin(val_positive_clients), 'target'] = 1\n",
    "\n",
    "print(f\"\\nTrain dataset:\")\n",
    "print(f\"  Total clients: {len(train_targets):,}\")\n",
    "print(f\"  Positive (opened deposit): {train_targets['target'].sum():,} ({train_targets['target'].mean()*100:.2f}%)\")\n",
    "print(f\"  Negative (did not open): {(train_targets['target']==0).sum():,} ({(1-train_targets['target'].mean())*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nValidation dataset:\")\n",
    "print(f\"  Total clients: {len(val_targets):,}\")\n",
    "print(f\"  Positive (opened deposit): {val_targets['target'].sum():,} ({val_targets['target'].mean()*100:.2f}%)\")\n",
    "print(f\"  Negative (did not open): {(val_targets['target']==0).sum():,} ({(1-val_targets['target'].mean())*100:.2f}%)\")\n",
    "\n",
    "# 2.5 Check for data leakage between train and val\n",
    "print(\"\\n2.5 Checking for temporal overlap (leakage detection)...\")\n",
    "overlap_clients = set(train_positive_clients) & set(val_positive_clients)\n",
    "print(f\"  Clients who opened deposits in BOTH Feb and Mar: {len(overlap_clients):,}\")\n",
    "print(f\"  This is OK - these are repeat customers (not leakage)\")\n",
    "\n",
    "# Critical check: Ensure we're not using future data\n",
    "train_max_date = deposits[deposits['date_rep'] <= train_observation_end]['date_rep'].max()\n",
    "val_max_date = deposits[deposits['date_rep'] <= val_observation_end]['date_rep'].max()\n",
    "print(f\"\\n  Latest date_rep we'll use for train features: {train_max_date}\")\n",
    "print(f\"  Latest date_rep we'll use for val features: {val_max_date}\")\n",
    "print(f\"  Train prediction starts: {train_prediction_start}\")\n",
    "print(f\"  Val prediction starts: {val_prediction_start}\")\n",
    "\n",
    "if train_max_date < pd.Timestamp(train_prediction_start):\n",
    "    print(\"  ✓ No temporal leakage in train split\")\n",
    "else:\n",
    "    print(\"  ❌ WARNING: Potential leakage in train split!\")\n",
    "\n",
    "if val_max_date < pd.Timestamp(val_prediction_start):\n",
    "    print(\"  ✓ No temporal leakage in val split\")\n",
    "else:\n",
    "    print(\"  ❌ WARNING: Potential leakage in val split!\")\n",
    "\n",
    "print(\"\\n✓ Temporal splits and targets defined!\")\n",
    "\n",
    "# Store for later use\n",
    "temporal_config = {\n",
    "    'train': {\n",
    "        'observation_start': train_observation_start,\n",
    "        'observation_end': train_observation_end,\n",
    "        'prediction_start': train_prediction_start,\n",
    "        'prediction_end': train_prediction_end\n",
    "    },\n",
    "    'val': {\n",
    "        'observation_start': val_observation_start,\n",
    "        'observation_end': val_observation_end,\n",
    "        'prediction_start': val_prediction_start,\n",
    "        'prediction_end': val_prediction_end\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf80c3b",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "27712c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 3: FEATURE ENGINEERING - INCLUDING FIRST-TIME OPENERS\n",
      "============================================================\n",
      "\n",
      "This may take a few minutes...\n",
      "\n",
      "Creating features for TRAIN (observation end: 2025-01-31)...\n",
      "  Processing clients 0 to 5,000 of 30,630...\n",
      "  Processing clients 10,000 to 15,000 of 30,630...\n",
      "  Processing clients 20,000 to 25,000 of 30,630...\n",
      "  Processing clients 30,000 to 30,630 of 30,630...\n",
      "  ✓ Created 25 features for 30,630 clients\n",
      "  ✓ First-time opener candidates: 72 (0.2%)\n",
      "\n",
      "Creating features for VALIDATION (observation end: 2025-02-28)...\n",
      "  Processing clients 0 to 5,000 of 31,260...\n",
      "  Processing clients 10,000 to 15,000 of 31,260...\n",
      "  Processing clients 20,000 to 25,000 of 31,260...\n",
      "  Processing clients 30,000 to 31,260 of 31,260...\n",
      "  ✓ Created 25 features for 31,260 clients\n",
      "  ✓ First-time opener candidates: 82 (0.3%)\n",
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING SUMMARY\n",
      "============================================================\n",
      "\n",
      "Train features shape: (30630, 26)\n",
      "Val features shape: (31260, 26)\n",
      "\n",
      "New feature added: 'has_deposit_history' (1=existing customer, 0=first-timer)\n",
      "\n",
      "✓ Feature engineering completed with first-time openers included!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3: FEATURE ENGINEERING (INCLUDES FIRST-TIME OPENERS)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: FEATURE ENGINEERING - INCLUDING FIRST-TIME OPENERS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def create_features_for_split(client_ids, observation_end_date, split_name):\n",
    "    \"\"\"\n",
    "    Create features for a list of clients using ONLY data up to observation_end_date.\n",
    "    NOW INCLUDES: Clients without prior deposit history (first-time openers)\n",
    "    \"\"\"\n",
    "    print(f\"\\nCreating features for {split_name} (observation end: {observation_end_date})...\")\n",
    "    \n",
    "    features_list = []\n",
    "    observation_end = pd.Timestamp(observation_end_date)\n",
    "    \n",
    "    # Process in batches for progress tracking\n",
    "    batch_size = 5000\n",
    "    total_clients = len(client_ids)\n",
    "    \n",
    "    for i in range(0, total_clients, batch_size):\n",
    "        batch_clients = client_ids[i:min(i+batch_size, total_clients)]\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(f\"  Processing clients {i:,} to {min(i+batch_size, total_clients):,} of {total_clients:,}...\")\n",
    "        \n",
    "        for client_id in batch_clients:\n",
    "            features = {'client_id': client_id}\n",
    "            \n",
    "            # ============================================\n",
    "            # A. CLIENT DEMOGRAPHICS (Static features)\n",
    "            # ============================================\n",
    "            client_demo = clients[clients['client_id'] == client_id]\n",
    "            if len(client_demo) > 0:\n",
    "                # Calculate age from birthday\n",
    "                birthday = client_demo['birthday'].values[0]\n",
    "                if pd.notna(birthday):\n",
    "                    age = (observation_end - pd.Timestamp(birthday)).days / 365.25\n",
    "                    features['age'] = int(age)\n",
    "                else:\n",
    "                    features['age'] = np.nan\n",
    "                \n",
    "                features['code_gender'] = client_demo['code_gender'].values[0]\n",
    "                features['code_class_credit'] = client_demo['code_class_credit'].values[0]\n",
    "                features['state'] = client_demo['state'].values[0]\n",
    "                features['code_citizenship'] = client_demo['code_citizenship'].values[0]\n",
    "                \n",
    "            else:\n",
    "                features.update({\n",
    "                    'age': np.nan,\n",
    "                    'code_gender': np.nan,\n",
    "                    'code_class_credit': np.nan,\n",
    "                    'state': np.nan,\n",
    "                    'code_citizenship': np.nan\n",
    "                })\n",
    "            \n",
    "            # ============================================\n",
    "            # B. DEPOSIT HISTORY (Historical features)\n",
    "            # ============================================\n",
    "            # CRITICAL CHANGE: Only exclude deposits opened AFTER observation_end\n",
    "            # Allow deposits opened ON observation_end for first-time openers\n",
    "            # But use snapshots BEFORE observation_end to avoid leakage\n",
    "            \n",
    "            hist_deposits = deposits[\n",
    "                (deposits['client_id'] == client_id) &\n",
    "                (deposits['date_rep'] < observation_end) &  # Use snapshots before observation\n",
    "                (deposits['date_open'] < observation_end)   # Only deposits opened before\n",
    "            ]\n",
    "            \n",
    "            if len(hist_deposits) > 0:\n",
    "                # Client HAS deposit history\n",
    "                latest_snapshots = hist_deposits.sort_values('date_rep').groupby('date_open').tail(1)\n",
    "                \n",
    "                features['num_deposits_ever'] = latest_snapshots['date_open'].nunique()\n",
    "                features['num_active_deposits'] = (latest_snapshots['state_sd'] == 1).sum()\n",
    "                \n",
    "                active_deposits = latest_snapshots[latest_snapshots['state_sd'] == 1]\n",
    "                features['total_balance'] = active_deposits['ostatok_op'].sum() if len(active_deposits) > 0 else 0\n",
    "                features['avg_deposit_amount'] = latest_snapshots['s_ost_vkl'].mean()\n",
    "                \n",
    "                first_deposit_date = hist_deposits['date_open'].min()\n",
    "                features['days_since_first_deposit'] = (observation_end - first_deposit_date).days\n",
    "                \n",
    "                last_deposit_date = hist_deposits['date_open'].max()\n",
    "                features['days_since_last_deposit'] = (observation_end - last_deposit_date).days\n",
    "                \n",
    "                features['avg_interest_rate'] = latest_snapshots['proc_all'].mean()\n",
    "                features['ever_closed_deposit'] = int((latest_snapshots['state_sd'] == 3).any())\n",
    "                \n",
    "                if features['days_since_first_deposit'] > 0:\n",
    "                    features['deposit_frequency_per_year'] = (features['num_deposits_ever'] / \n",
    "                                                               features['days_since_first_deposit']) * 365\n",
    "                else:\n",
    "                    features['deposit_frequency_per_year'] = 0\n",
    "                \n",
    "                features['avg_deposit_balance'] = active_deposits['ostatok_op'].mean() if len(active_deposits) > 0 else 0\n",
    "                \n",
    "                # Flag: has deposit history\n",
    "                features['has_deposit_history'] = 1\n",
    "                \n",
    "            else:\n",
    "                # Client has NO deposit history (first-time opener candidate)\n",
    "                features.update({\n",
    "                    'num_deposits_ever': 0,\n",
    "                    'num_active_deposits': 0,\n",
    "                    'total_balance': 0,\n",
    "                    'avg_deposit_amount': 0,\n",
    "                    'days_since_first_deposit': -1,  # Flag for no history\n",
    "                    'days_since_last_deposit': -1,\n",
    "                    'avg_interest_rate': 0,\n",
    "                    'ever_closed_deposit': 0,\n",
    "                    'deposit_frequency_per_year': 0,\n",
    "                    'avg_deposit_balance': 0,\n",
    "                    'has_deposit_history': 0  # Flag: first-time opener candidate\n",
    "                })\n",
    "            \n",
    "            # ============================================\n",
    "            # C. TRANSACTION BEHAVIOR (Last 30/90 days)\n",
    "            # ============================================\n",
    "            client_trans = transactions[\n",
    "                (transactions['client_id'] == client_id) &\n",
    "                (transactions['v_date'] <= observation_end)\n",
    "            ]\n",
    "            \n",
    "            if len(client_trans) > 0:\n",
    "                # Last 30 days\n",
    "                trans_30d = client_trans[\n",
    "                    client_trans['v_date'] > (observation_end - pd.Timedelta(days=30))\n",
    "                ]\n",
    "                features['trans_count_30d'] = len(trans_30d)\n",
    "                features['trans_volume_30d'] = trans_30d['dt'].sum() if len(trans_30d) > 0 else 0\n",
    "                features['trans_avg_amount_30d'] = trans_30d['dt'].mean() if len(trans_30d) > 0 else 0\n",
    "                \n",
    "                # Last 90 days\n",
    "                trans_90d = client_trans[\n",
    "                    client_trans['v_date'] > (observation_end - pd.Timedelta(days=90))\n",
    "                ]\n",
    "                features['trans_count_90d'] = len(trans_90d)\n",
    "                features['trans_volume_90d'] = trans_90d['dt'].sum() if len(trans_90d) > 0 else 0\n",
    "                \n",
    "                # Debit vs Credit ratio\n",
    "                total_debit = trans_90d['dt'].sum()\n",
    "                total_credit = trans_90d['ct'].sum()\n",
    "                if total_credit > 0:\n",
    "                    features['debit_credit_ratio'] = total_debit / total_credit\n",
    "                else:\n",
    "                    features['debit_credit_ratio'] = 0\n",
    "                \n",
    "                # Transaction trend\n",
    "                trans_60_90d = client_trans[\n",
    "                    (client_trans['v_date'] > (observation_end - pd.Timedelta(days=90))) &\n",
    "                    (client_trans['v_date'] <= (observation_end - pd.Timedelta(days=30)))\n",
    "                ]\n",
    "                trans_count_60_90d = len(trans_60_90d)\n",
    "                if trans_count_60_90d > 0:\n",
    "                    features['trans_trend'] = features['trans_count_30d'] / trans_count_60_90d\n",
    "                else:\n",
    "                    features['trans_trend'] = 1.0\n",
    "                \n",
    "            else:\n",
    "                features.update({\n",
    "                    'trans_count_30d': 0,\n",
    "                    'trans_volume_30d': 0,\n",
    "                    'trans_avg_amount_30d': 0,\n",
    "                    'trans_count_90d': 0,\n",
    "                    'trans_volume_90d': 0,\n",
    "                    'debit_credit_ratio': 0,\n",
    "                    'trans_trend': 1.0\n",
    "                })\n",
    "            \n",
    "            # ============================================\n",
    "            # D. FIREBASE ENGAGEMENT (if available)\n",
    "            # ============================================\n",
    "            if observation_end <= pd.Timestamp('2025-01-31'):\n",
    "                client_firebase = firebase[\n",
    "                    (firebase['client_id'] == client_id) &\n",
    "                    (firebase['event_date'] <= observation_end)\n",
    "                ]\n",
    "                \n",
    "                if len(client_firebase) > 0:\n",
    "                    firebase_30d = client_firebase[\n",
    "                        client_firebase['event_date'] > (observation_end - pd.Timedelta(days=30))\n",
    "                    ]\n",
    "                    features['firebase_event_count_30d'] = len(firebase_30d)\n",
    "                    features['firebase_unique_events_30d'] = firebase_30d['event_name'].nunique() if len(firebase_30d) > 0 else 0\n",
    "                else:\n",
    "                    features['firebase_event_count_30d'] = 0\n",
    "                    features['firebase_unique_events_30d'] = 0\n",
    "            else:\n",
    "                features['firebase_event_count_30d'] = np.nan\n",
    "                features['firebase_unique_events_30d'] = np.nan\n",
    "            \n",
    "            features_list.append(features)\n",
    "    \n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    # Report on first-time openers\n",
    "    first_timers = (features_df['has_deposit_history'] == 0).sum()\n",
    "    print(f\"  ✓ Created {len(features_df.columns)-1} features for {len(features_df):,} clients\")\n",
    "    print(f\"  ✓ First-time opener candidates: {first_timers:,} ({first_timers/len(features_df)*100:.1f}%)\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "\n",
    "# Create features for train and validation sets\n",
    "print(\"\\nThis may take a few minutes...\")\n",
    "train_features = create_features_for_split(\n",
    "    train_targets['client_id'].values,\n",
    "    train_observation_end,\n",
    "    'TRAIN'\n",
    ")\n",
    "\n",
    "val_features = create_features_for_split(\n",
    "    val_targets['client_id'].values,\n",
    "    val_observation_end,\n",
    "    'VALIDATION'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTrain features shape: {train_features.shape}\")\n",
    "print(f\"Val features shape: {val_features.shape}\")\n",
    "print(f\"\\nNew feature added: 'has_deposit_history' (1=existing customer, 0=first-timer)\")\n",
    "\n",
    "print(\"\\n✓ Feature engineering completed with first-time openers included!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28979106",
   "metadata": {},
   "source": [
    "Merge Features with Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55638917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 4: MERGE FEATURES WITH TARGETS\n",
      "============================================================\n",
      "\n",
      "Train dataset shape: (30630, 27)\n",
      "Val dataset shape: (31260, 27)\n",
      "\n",
      "Target distribution in TRAIN:\n",
      "target\n",
      "0    27984\n",
      "1     2646\n",
      "Name: count, dtype: int64\n",
      "Positive rate: 8.64%\n",
      "\n",
      "Target distribution in VAL:\n",
      "target\n",
      "0    28581\n",
      "1     2679\n",
      "Name: count, dtype: int64\n",
      "Positive rate: 8.57%\n",
      "\n",
      "✓ Features and targets merged!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 4: MERGE FEATURES WITH TARGETS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4: MERGE FEATURES WITH TARGETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Merge features with target labels\n",
    "train_df = train_features.merge(train_targets, on='client_id', how='inner')\n",
    "val_df = val_features.merge(val_targets, on='client_id', how='inner')\n",
    "\n",
    "print(f\"\\nTrain dataset shape: {train_df.shape}\")\n",
    "print(f\"Val dataset shape: {val_df.shape}\")\n",
    "\n",
    "print(f\"\\nTarget distribution in TRAIN:\")\n",
    "print(train_df['target'].value_counts())\n",
    "print(f\"Positive rate: {train_df['target'].mean()*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTarget distribution in VAL:\")\n",
    "print(val_df['target'].value_counts())\n",
    "print(f\"Positive rate: {val_df['target'].mean()*100:.2f}%\")\n",
    "\n",
    "print(\"\\n✓ Features and targets merged!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345fbc4",
   "metadata": {},
   "source": [
    "Handle Missing Values & Prepare Final Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bb09617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 5 & 6: PREPARE FINAL MODELING DATASETS\n",
      "============================================================\n",
      "\n",
      "5.1 Handling missing values...\n",
      "Missing values after imputation (TRAIN): 0\n",
      "Missing values after imputation (VAL): 0\n",
      "\n",
      "5.2 Separating features (X) and target (y)...\n",
      "\n",
      "X_train shape: (30630, 25)\n",
      "y_train shape: (30630,)\n",
      "X_val shape: (31260, 25)\n",
      "y_val shape: (31260,)\n",
      "\n",
      "Categorical features: 4\n",
      "Numeric features: 21\n",
      "\n",
      "============================================================\n",
      "FINAL DATASET SUMMARY\n",
      "============================================================\n",
      "\n",
      "TRAIN SET:\n",
      "  Total samples: 30,630\n",
      "  Positive class: 2,646 (8.64%)\n",
      "  Negative class: 27,984 (91.36%)\n",
      "  Features: 25\n",
      "\n",
      "VALIDATION SET:\n",
      "  Total samples: 31,260\n",
      "  Positive class: 2,679 (8.57%)\n",
      "  Negative class: 28,581 (91.43%)\n",
      "  Features: 25\n",
      "\n",
      "✓ Datasets ready for XGBoost training!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 5 & 6: HANDLE MISSING VALUES & PREPARE DATASETS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 5 & 6: PREPARE FINAL MODELING DATASETS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\n5.1 Handling missing values...\")\n",
    "\n",
    "fill_values = {\n",
    "    'days_since_first_deposit': -1,  # -1 indicates no history\n",
    "    'days_since_last_deposit': -1,\n",
    "    'avg_interest_rate': 0,\n",
    "    'firebase_event_count_30d': 0,\n",
    "    'firebase_unique_events_30d': 0\n",
    "}\n",
    "\n",
    "train_df_clean = train_df.fillna(fill_values)\n",
    "val_df_clean = val_df.fillna(fill_values)\n",
    "\n",
    "print(f\"Missing values after imputation (TRAIN): {train_df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Missing values after imputation (VAL): {val_df_clean.isnull().sum().sum()}\")\n",
    "\n",
    "# Separate features and target\n",
    "print(\"\\n5.2 Separating features (X) and target (y)...\")\n",
    "\n",
    "feature_cols = [col for col in train_df_clean.columns \n",
    "                if col not in ['client_id', 'target']]\n",
    "\n",
    "X_train = train_df_clean[feature_cols]\n",
    "y_train = train_df_clean['target']\n",
    "train_client_ids = train_df_clean['client_id']\n",
    "\n",
    "X_val = val_df_clean[feature_cols]\n",
    "y_val = val_df_clean['target']\n",
    "val_client_ids = val_df_clean['client_id']\n",
    "\n",
    "print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "\n",
    "# Identify categorical vs numeric features\n",
    "categorical_features = ['code_gender', 'code_class_credit', 'state', 'code_citizenship']\n",
    "numeric_features = [col for col in feature_cols if col not in categorical_features]\n",
    "\n",
    "print(f\"\\nCategorical features: {len(categorical_features)}\")\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTRAIN SET:\")\n",
    "print(f\"  Total samples: {len(X_train):,}\")\n",
    "print(f\"  Positive class: {y_train.sum():,} ({y_train.mean()*100:.2f}%)\")\n",
    "print(f\"  Negative class: {(y_train==0).sum():,} ({(1-y_train.mean())*100:.2f}%)\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")\n",
    "\n",
    "print(f\"\\nVALIDATION SET:\")\n",
    "print(f\"  Total samples: {len(X_val):,}\")\n",
    "print(f\"  Positive class: {y_val.sum():,} ({y_val.mean()*100:.2f}%)\")\n",
    "print(f\"  Negative class: {(y_val==0).sum():,} ({(1-y_val.mean())*100:.2f}%)\")\n",
    "print(f\"  Features: {X_val.shape[1]}\")\n",
    "\n",
    "print(\"\\n✓ Datasets ready for XGBoost training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa14d51",
   "metadata": {},
   "source": [
    " XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd4cca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 7: XGBOOST MODEL TRAINING\n",
      "============================================================\n",
      "\n",
      "7.1 Encoding categorical features...\n",
      "  state: 1 unseen categories in val\n",
      "  code_citizenship: 1 unseen categories in val\n",
      "✓ Encoded 4 categorical features\n",
      "\n",
      "7.2 Class imbalance ratio: 10.58\n",
      "\n",
      "7.3 Training XGBoost model...\n",
      "[0]\tvalidation_0-auc:0.75202\tvalidation_1-auc:0.69162\n",
      "[20]\tvalidation_0-auc:0.81631\tvalidation_1-auc:0.74696\n",
      "[32]\tvalidation_0-auc:0.82531\tvalidation_1-auc:0.74794\n",
      "\n",
      "✓ Model trained with 13 trees\n",
      "\n",
      "7.4 Making predictions...\n",
      "✓ Predictions generated\n",
      "\n",
      "============================================================\n",
      "XGBOOST MODEL EVALUATION\n",
      "============================================================\n",
      "\n",
      "AUC-ROC:\n",
      "  Train: 0.8089\n",
      "  Val:   0.7497\n",
      "\n",
      "Average Precision:\n",
      "  Train: 0.3777\n",
      "  Val:   0.2783\n",
      "\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Deposit       0.95      0.80      0.87     28581\n",
      " New Deposit       0.21      0.58      0.31      2679\n",
      "\n",
      "    accuracy                           0.78     31260\n",
      "   macro avg       0.58      0.69      0.59     31260\n",
      "weighted avg       0.89      0.78      0.82     31260\n",
      "\n",
      "\n",
      "============================================================\n",
      "BUSINESS METRICS\n",
      "============================================================\n",
      "\n",
      "Top 5%:\n",
      "  Precision: 39.9%\n",
      "  Lift: 4.7x\n",
      "  Conversions: 623 / 1,563\n",
      "\n",
      "Top 10%:\n",
      "  Precision: 30.5%\n",
      "  Lift: 3.6x\n",
      "  Conversions: 953 / 3,126\n",
      "\n",
      "Top 20%:\n",
      "  Precision: 22.9%\n",
      "  Lift: 2.7x\n",
      "  Conversions: 1,433 / 6,252\n",
      "\n",
      "============================================================\n",
      "TOP 15 FEATURES\n",
      "============================================================\n",
      "                   feature  importance\n",
      "         num_deposits_ever    0.300912\n",
      "       num_active_deposits    0.108949\n",
      "       ever_closed_deposit    0.097626\n",
      "deposit_frequency_per_year    0.065858\n",
      "   days_since_last_deposit    0.063049\n",
      "  days_since_first_deposit    0.060125\n",
      "             total_balance    0.040007\n",
      "       avg_deposit_balance    0.031860\n",
      "  firebase_event_count_30d    0.024349\n",
      "         avg_interest_rate    0.023074\n",
      "          trans_volume_90d    0.022569\n",
      "           trans_count_30d    0.022248\n",
      "           trans_count_90d    0.022062\n",
      "        avg_deposit_amount    0.021016\n",
      "          trans_volume_30d    0.019702\n",
      "\n",
      "============================================================\n",
      "OVERFITTING CHECK\n",
      "============================================================\n",
      "AUC difference: 0.0592\n",
      "⚠️ Slight overfitting\n",
      "\n",
      "✓ XGBoost training complete!\n",
      "✓ Model saved to 'xgboost_nbo_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 7: XGBOOST MODEL\n",
    "# ============================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 7: XGBOOST MODEL TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 7.1 Encode categorical features\n",
    "print(\"\\n7.1 Encoding categorical features...\")\n",
    "\n",
    "X_train_encoded = X_train.copy()\n",
    "X_val_encoded = X_val.copy()\n",
    "\n",
    "# Handle unseen categories\n",
    "for col in categorical_features:\n",
    "    train_cats = set(X_train[col].unique())\n",
    "    val_cats = set(X_val[col].unique())\n",
    "    unseen = val_cats - train_cats\n",
    "    if len(unseen) > 0:\n",
    "        print(f\"  {col}: {len(unseen)} unseen categories in val\")\n",
    "        mode_val = X_train[col].mode()[0]\n",
    "        X_val_encoded.loc[X_val_encoded[col].isin(unseen), col] = mode_val\n",
    "\n",
    "# Encode\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    X_train_encoded[col] = le.fit_transform(X_train_encoded[col].fillna(-999).astype(str))\n",
    "    X_val_encoded[col] = le.transform(X_val_encoded[col].fillna(-999).astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "print(f\"✓ Encoded {len(categorical_features)} categorical features\")\n",
    "\n",
    "# 7.2 Calculate scale_pos_weight\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"\\n7.2 Class imbalance ratio: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# 7.3 Train XGBoost\n",
    "print(\"\\n7.3 Training XGBoost model...\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.05,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=5,\n",
    "    gamma=0.1,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    eval_metric='auc',\n",
    "    early_stopping_rounds=20,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "eval_set = [(X_train_encoded, y_train), (X_val_encoded, y_val)]\n",
    "xgb_model.fit(X_train_encoded, y_train, eval_set=eval_set, verbose=20)\n",
    "\n",
    "print(f\"\\n✓ Model trained with {xgb_model.best_iteration} trees\")\n",
    "\n",
    "# 7.4 Predictions\n",
    "print(\"\\n7.4 Making predictions...\")\n",
    "\n",
    "y_train_pred_proba = xgb_model.predict_proba(X_train_encoded)[:, 1]\n",
    "y_val_pred_proba = xgb_model.predict_proba(X_val_encoded)[:, 1]\n",
    "y_val_pred = xgb_model.predict(X_val_encoded)\n",
    "\n",
    "print(\"✓ Predictions generated\")\n",
    "\n",
    "# 7.5 Evaluation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_auc = roc_auc_score(y_train, y_train_pred_proba)\n",
    "val_auc = roc_auc_score(y_val, y_val_pred_proba)\n",
    "train_ap = average_precision_score(y_train, y_train_pred_proba)\n",
    "val_ap = average_precision_score(y_val, y_val_pred_proba)\n",
    "\n",
    "print(f\"\\nAUC-ROC:\")\n",
    "print(f\"  Train: {train_auc:.4f}\")\n",
    "print(f\"  Val:   {val_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage Precision:\")\n",
    "print(f\"  Train: {train_ap:.4f}\")\n",
    "print(f\"  Val:   {val_ap:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=['No Deposit', 'New Deposit']))\n",
    "\n",
    "# Business metrics\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BUSINESS METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for pct in [5, 10, 20]:\n",
    "    top_pct_n = int(len(y_val) * (pct/100))\n",
    "    top_indices = y_val_pred_proba.argsort()[-top_pct_n:]\n",
    "    precision = y_val.iloc[top_indices].mean()\n",
    "    lift = precision / y_val.mean()\n",
    "    \n",
    "    print(f\"\\nTop {pct}%:\")\n",
    "    print(f\"  Precision: {precision:.1%}\")\n",
    "    print(f\"  Lift: {lift:.1f}x\")\n",
    "    print(f\"  Conversions: {int(top_pct_n * precision):,} / {top_pct_n:,}\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TOP 15 FEATURES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Overfitting check\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OVERFITTING CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"AUC difference: {train_auc - val_auc:.4f}\")\n",
    "if abs(train_auc - val_auc) < 0.05:\n",
    "    print(\"✓ No significant overfitting\")\n",
    "elif abs(train_auc - val_auc) < 0.10:\n",
    "    print(\"⚠️ Slight overfitting\")\n",
    "else:\n",
    "    print(\"❌ Significant overfitting\")\n",
    "\n",
    "print(\"\\n✓ XGBoost training complete!\")\n",
    "\n",
    "# Save model\n",
    "import pickle\n",
    "with open('xgboost_nbo_model.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'model': xgb_model,\n",
    "        'label_encoders': label_encoders,\n",
    "        'feature_cols': feature_cols,\n",
    "        'categorical_features': categorical_features\n",
    "    }, f)\n",
    "print(\"✓ Model saved to 'xgboost_nbo_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52665f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                              FINAL RESULTS\n",
      "================================================================================\n",
      "\n",
      "📊 MODEL PERFORMANCE:\n",
      "   Validation AUC: 0.7497\n",
      "   Validation AP:  0.2783\n",
      "   Training iterations: 13\n",
      "\n",
      "💼 BUSINESS METRICS:\n",
      "\n",
      "   Top 5% Targeting:\n",
      "      Precision: 39.9%\n",
      "      Lift: 4.7x\n",
      "      Expected conversions: 623 out of 1,563\n",
      "\n",
      "   Top 10% Targeting:\n",
      "      Precision: 30.5%\n",
      "      Lift: 3.6x\n",
      "      Expected conversions: 953 out of 3,126\n",
      "\n",
      "   Top 20% Targeting:\n",
      "      Precision: 22.9%\n",
      "      Lift: 2.7x\n",
      "      Expected conversions: 1,433 out of 6,252\n",
      "\n",
      "🎯 CONFUSION MATRIX:\n",
      "   True Negatives:  22,757\n",
      "   False Positives: 5,824\n",
      "   False Negatives: 1,135\n",
      "   True Positives:  1,544\n",
      "\n",
      "   Recall: 57.6%\n",
      "   Precision: 21.0%\n",
      "\n",
      "🔍 TOP 10 FEATURES:\n",
      "   num_deposits_ever              0.3009\n",
      "   num_active_deposits            0.1089\n",
      "   ever_closed_deposit            0.0976\n",
      "   deposit_frequency_per_year     0.0659\n",
      "   days_since_last_deposit        0.0630\n",
      "   days_since_first_deposit       0.0601\n",
      "   total_balance                  0.0400\n",
      "   avg_deposit_balance            0.0319\n",
      "   firebase_event_count_30d       0.0243\n",
      "   avg_interest_rate              0.0231\n",
      "\n",
      "⚖️ OVERFITTING:\n",
      "   Train AUC: 0.8089\n",
      "   Val AUC:   0.7497\n",
      "   Difference: 0.0592\n",
      "\n",
      "================================================================================\n",
      "FINAL VERDICT: ⚠️ BORDERLINE\n",
      "Validation AUC: 0.7497\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\" \"*30 + \"FINAL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n📊 MODEL PERFORMANCE:\")\n",
    "print(f\"   Validation AUC: {val_auc:.4f}\")\n",
    "print(f\"   Validation AP:  {val_ap:.4f}\")\n",
    "print(f\"   Training iterations: {xgb_model.best_iteration}\")\n",
    "\n",
    "print(f\"\\n💼 BUSINESS METRICS:\")\n",
    "for pct in [5, 10, 20]:\n",
    "    top_pct_n = int(len(y_val) * (pct/100))\n",
    "    top_indices = y_val_pred_proba.argsort()[-top_pct_n:]\n",
    "    precision = y_val.iloc[top_indices].mean()\n",
    "    lift = precision / y_val.mean()\n",
    "    conversions = int(top_pct_n * precision)\n",
    "    \n",
    "    print(f\"\\n   Top {pct}% Targeting:\")\n",
    "    print(f\"      Precision: {precision:.1%}\")\n",
    "    print(f\"      Lift: {lift:.1f}x\")\n",
    "    print(f\"      Expected conversions: {conversions:,} out of {top_pct_n:,}\")\n",
    "\n",
    "print(f\"\\n🎯 CONFUSION MATRIX:\")\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(f\"   True Negatives:  {cm[0,0]:,}\")\n",
    "print(f\"   False Positives: {cm[0,1]:,}\")\n",
    "print(f\"   False Negatives: {cm[1,0]:,}\")\n",
    "print(f\"   True Positives:  {cm[1,1]:,}\")\n",
    "\n",
    "recall = cm[1,1] / (cm[1,0] + cm[1,1])\n",
    "precision_pos = cm[1,1] / (cm[0,1] + cm[1,1])\n",
    "print(f\"\\n   Recall: {recall:.1%}\")\n",
    "print(f\"   Precision: {precision_pos:.1%}\")\n",
    "\n",
    "print(f\"\\n🔍 TOP 10 FEATURES:\")\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"   {row['feature']:<30} {row['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n⚖️ OVERFITTING:\")\n",
    "print(f\"   Train AUC: {train_auc:.4f}\")\n",
    "print(f\"   Val AUC:   {val_auc:.4f}\")\n",
    "print(f\"   Difference: {train_auc - val_auc:.4f}\")\n",
    "\n",
    "if val_auc >= 0.75:\n",
    "    status = \"✅ PRODUCTION READY\"\n",
    "elif val_auc >= 0.70:\n",
    "    status = \"⚠️ BORDERLINE\"\n",
    "else:\n",
    "    status = \"❌ NEEDS WORK\"\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"FINAL VERDICT: {status}\")\n",
    "print(f\"Validation AUC: {val_auc:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b72fb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUICK REASONING CHECK\n",
      "================================================================================\n",
      "\n",
      "📊 WHAT DRIVES PREDICTIONS:\n",
      "   Deposit history:     84.3%\n",
      "   Transaction activity: 11.4%\n",
      "   Digital engagement:   4.2%\n",
      "\n",
      "📊 HIGH SCORERS vs LOW SCORERS:\n",
      "   num_deposits_ever: Top=4.4 vs Bottom=1.0\n",
      "   trans_count_30d: Top=0.0 vs Bottom=0.0\n",
      "   total_balance: Top=287302035.2 vs Bottom=30429846.3\n",
      "\n",
      "✅ BUSINESS LOGIC:\n",
      "   More deposits → Higher score: 0.467 correlation\n",
      "   More transactions → Higher score: nan correlation\n",
      "   ✅ Model correctly targets repeat customers\n",
      "\n",
      "🔍 LEAKAGE CHECK:\n",
      "   Max prediction: 0.7371\n",
      "   Min prediction: 0.3022\n",
      "   ✅ No suspiciously high confidence\n",
      "   Max feature-target correlation: 0.2059\n",
      "   ✅ No leakage detected\n",
      "\n",
      "🎯 SAMPLE HIGH SCORER:\n",
      "   Prediction score: 0.7371\n",
      "   Actually opened: YES ✅\n",
      "   num_deposits_ever: 22\n",
      "   trans_count_30d: 0\n",
      "   total_balance: 42500000\n",
      "\n",
      "📉 SAMPLE LOW SCORER:\n",
      "   Prediction score: 0.3022\n",
      "   Actually opened: NO ❌\n",
      "   num_deposits_ever: 1\n",
      "   trans_count_30d: 0\n",
      "   total_balance: 500018\n",
      "\n",
      "================================================================================\n",
      "VERDICT: Does the model make sense?\n",
      "================================================================================\n",
      "✅ MODEL REASONING IS SOUND\n",
      "   - Uses deposit history logically\n",
      "   - No leakage detected\n",
      "   - Confidence levels reasonable\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"QUICK REASONING CHECK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Feature breakdown\n",
    "print(\"\\n📊 WHAT DRIVES PREDICTIONS:\")\n",
    "deposit_imp = feature_importance[feature_importance['feature'].str.contains('deposit', case=False)]['importance'].sum()\n",
    "trans_imp = feature_importance[feature_importance['feature'].str.contains('trans', case=False)]['importance'].sum()\n",
    "firebase_imp = feature_importance[feature_importance['feature'].str.contains('firebase', case=False)]['importance'].sum()\n",
    "\n",
    "total = deposit_imp + trans_imp + firebase_imp\n",
    "print(f\"   Deposit history:     {deposit_imp/total*100:.1f}%\")\n",
    "print(f\"   Transaction activity: {trans_imp/total*100:.1f}%\")\n",
    "print(f\"   Digital engagement:   {firebase_imp/total*100:.1f}%\")\n",
    "\n",
    "# 2. Top vs Bottom comparison\n",
    "print(\"\\n📊 HIGH SCORERS vs LOW SCORERS:\")\n",
    "top_10_idx = y_val_pred_proba.argsort()[-int(len(y_val)*0.1):]\n",
    "bottom_10_idx = y_val_pred_proba.argsort()[:int(len(y_val)*0.1)]\n",
    "\n",
    "key_features = ['num_deposits_ever', 'trans_count_30d', 'total_balance']\n",
    "for feat in key_features:\n",
    "    top_mean = X_val.iloc[top_10_idx][feat].mean()\n",
    "    bottom_mean = X_val.iloc[bottom_10_idx][feat].mean()\n",
    "    print(f\"   {feat}: Top={top_mean:.1f} vs Bottom={bottom_mean:.1f}\")\n",
    "\n",
    "# 3. Business logic check\n",
    "print(\"\\n✅ BUSINESS LOGIC:\")\n",
    "corr_deposits = np.corrcoef(X_val_encoded['num_deposits_ever'], y_val_pred_proba)[0,1]\n",
    "corr_trans = np.corrcoef(X_val_encoded['trans_count_30d'], y_val_pred_proba)[0,1]\n",
    "print(f\"   More deposits → Higher score: {corr_deposits:.3f} correlation\")\n",
    "print(f\"   More transactions → Higher score: {corr_trans:.3f} correlation\")\n",
    "\n",
    "if corr_deposits > 0.15:\n",
    "    print(\"   ✅ Model correctly targets repeat customers\")\n",
    "else:\n",
    "    print(\"   ⚠️ Model not using deposit history strongly\")\n",
    "\n",
    "# 4. Leakage check\n",
    "print(\"\\n🔍 LEAKAGE CHECK:\")\n",
    "print(f\"   Max prediction: {y_val_pred_proba.max():.4f}\")\n",
    "print(f\"   Min prediction: {y_val_pred_proba.min():.4f}\")\n",
    "\n",
    "if y_val_pred_proba.max() > 0.95:\n",
    "    print(\"   ⚠️ SUSPICIOUS - Overconfident predictions!\")\n",
    "else:\n",
    "    print(\"   ✅ No suspiciously high confidence\")\n",
    "\n",
    "# Check for perfect correlations\n",
    "max_corr = 0\n",
    "for feat in numeric_features[:10]:  # Check top 10\n",
    "    if feat in X_val.columns:\n",
    "        corr = abs(X_val[feat].corr(y_val))\n",
    "        max_corr = max(max_corr, corr)\n",
    "\n",
    "print(f\"   Max feature-target correlation: {max_corr:.4f}\")\n",
    "if max_corr > 0.5:\n",
    "    print(\"   ⚠️ SUSPICIOUS - Possible leakage!\")\n",
    "else:\n",
    "    print(\"   ✅ No leakage detected\")\n",
    "\n",
    "# 5. Sample high scorer\n",
    "print(\"\\n🎯 SAMPLE HIGH SCORER:\")\n",
    "top_idx = y_val_pred_proba.argmax()\n",
    "print(f\"   Prediction score: {y_val_pred_proba[top_idx]:.4f}\")\n",
    "print(f\"   Actually opened: {'YES ✅' if y_val.iloc[top_idx] == 1 else 'NO ❌'}\")\n",
    "print(f\"   num_deposits_ever: {X_val.iloc[top_idx]['num_deposits_ever']:.0f}\")\n",
    "print(f\"   trans_count_30d: {X_val.iloc[top_idx]['trans_count_30d']:.0f}\")\n",
    "print(f\"   total_balance: {X_val.iloc[top_idx]['total_balance']:.0f}\")\n",
    "\n",
    "# 6. Sample low scorer\n",
    "print(\"\\n📉 SAMPLE LOW SCORER:\")\n",
    "bottom_idx = y_val_pred_proba.argmin()\n",
    "print(f\"   Prediction score: {y_val_pred_proba[bottom_idx]:.4f}\")\n",
    "print(f\"   Actually opened: {'YES ✅' if y_val.iloc[bottom_idx] == 1 else 'NO ❌'}\")\n",
    "print(f\"   num_deposits_ever: {X_val.iloc[bottom_idx]['num_deposits_ever']:.0f}\")\n",
    "print(f\"   trans_count_30d: {X_val.iloc[bottom_idx]['trans_count_30d']:.0f}\")\n",
    "print(f\"   total_balance: {X_val.iloc[bottom_idx]['total_balance']:.0f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERDICT: Does the model make sense?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "issues = []\n",
    "if corr_deposits < 0.1:\n",
    "    issues.append(\"Not using deposit history strongly\")\n",
    "if y_val_pred_proba.max() > 0.95:\n",
    "    issues.append(\"Overconfident predictions\")\n",
    "if max_corr > 0.5:\n",
    "    issues.append(\"Possible data leakage\")\n",
    "\n",
    "if len(issues) == 0:\n",
    "    print(\"✅ MODEL REASONING IS SOUND\")\n",
    "    print(\"   - Uses deposit history logically\")\n",
    "    print(\"   - No leakage detected\")\n",
    "    print(\"   - Confidence levels reasonable\")\n",
    "else:\n",
    "    print(\"⚠️ POTENTIAL ISSUES:\")\n",
    "    for issue in issues:\n",
    "        print(f\"   - {issue}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
